{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to C:\\Users\n",
      "[nltk_data]    |     \\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to C:\\Users\\m\n",
      "[nltk_data]    |     uhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\muhammadabunaser\\A\n",
      "[nltk_data]    |     ppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to C:\\Use\n",
      "[nltk_data]    |     rs\\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to \n",
      "[nltk_data]    |     C:\\Users\\muhammadabunaser\\AppData\\Roaming\\nltk_da\n",
      "[nltk_data]    |     ta...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to C:\\Users\\\n",
      "[nltk_data]    |     muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27290, 50)\n",
      "Train on 16374 samples, validate on 10916 samples\n",
      "Epoch 1/100\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.4722 - acc: 0.7752 - val_loss: 0.4325 - val_acc: 0.8073\n",
      "Epoch 2/100\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.3290 - acc: 0.8642 - val_loss: 0.4562 - val_acc: 0.7818\n",
      "Epoch 3/100\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.2558 - acc: 0.8987 - val_loss: 0.4882 - val_acc: 0.7962\n",
      "Epoch 4/100\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.2027 - acc: 0.9211 - val_loss: 0.6035 - val_acc: 0.7868\n",
      "Epoch 5/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.1598 - acc: 0.9392 - val_loss: 0.6722 - val_acc: 0.7827\n",
      "Epoch 6/100\n",
      "16374/16374 [==============================] - 69s 4ms/step - loss: 0.1349 - acc: 0.9487 - val_loss: 0.6887 - val_acc: 0.7752\n",
      "Epoch 7/100\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.1054 - acc: 0.9623 - val_loss: 0.8096 - val_acc: 0.7705\n",
      "Epoch 8/100\n",
      "16374/16374 [==============================] - 64s 4ms/step - loss: 0.0866 - acc: 0.9709 - val_loss: 0.9481 - val_acc: 0.7673\n",
      "Epoch 9/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.0732 - acc: 0.9744 - val_loss: 0.9823 - val_acc: 0.7688\n",
      "Epoch 10/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.0600 - acc: 0.9797 - val_loss: 1.1176 - val_acc: 0.7622\n",
      "Epoch 11/100\n",
      "16374/16374 [==============================] - 79s 5ms/step - loss: 0.0469 - acc: 0.9847 - val_loss: 1.1621 - val_acc: 0.7615\n",
      "Epoch 12/100\n",
      "16374/16374 [==============================] - 94s 6ms/step - loss: 0.0463 - acc: 0.9853 - val_loss: 1.1633 - val_acc: 0.7616\n",
      "Epoch 13/100\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 0.0371 - acc: 0.9878 - val_loss: 1.2068 - val_acc: 0.7652\n",
      "Epoch 14/100\n",
      "16374/16374 [==============================] - 79s 5ms/step - loss: 0.0276 - acc: 0.9916 - val_loss: 1.2692 - val_acc: 0.7594\n",
      "Epoch 15/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0264 - acc: 0.9911 - val_loss: 1.4129 - val_acc: 0.7560\n",
      "Epoch 16/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0261 - acc: 0.9920 - val_loss: 1.2948 - val_acc: 0.7582\n",
      "Epoch 17/100\n",
      "16374/16374 [==============================] - 65s 4ms/step - loss: 0.0258 - acc: 0.9914 - val_loss: 1.3792 - val_acc: 0.7626\n",
      "Epoch 18/100\n",
      "16374/16374 [==============================] - 79s 5ms/step - loss: 0.0238 - acc: 0.9925 - val_loss: 1.1996 - val_acc: 0.7541\n",
      "Epoch 19/100\n",
      "16374/16374 [==============================] - 88s 5ms/step - loss: 0.0164 - acc: 0.9947 - val_loss: 1.5596 - val_acc: 0.7578\n",
      "Epoch 20/100\n",
      "16374/16374 [==============================] - 74s 5ms/step - loss: 0.0160 - acc: 0.9948 - val_loss: 1.5254 - val_acc: 0.7564\n",
      "Epoch 21/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.0147 - acc: 0.9949 - val_loss: 1.5536 - val_acc: 0.7549\n",
      "Epoch 22/100\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.0117 - acc: 0.9964 - val_loss: 1.5960 - val_acc: 0.7478\n",
      "Epoch 23/100\n",
      "16374/16374 [==============================] - 68s 4ms/step - loss: 0.0111 - acc: 0.9965 - val_loss: 1.5430 - val_acc: 0.7465\n",
      "Epoch 24/100\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 1.5419 - val_acc: 0.7486\n",
      "Epoch 25/100\n",
      "16374/16374 [==============================] - 81s 5ms/step - loss: 0.0091 - acc: 0.9971 - val_loss: 1.6259 - val_acc: 0.7564\n",
      "Epoch 26/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0089 - acc: 0.9971 - val_loss: 1.7059 - val_acc: 0.7489\n",
      "Epoch 27/100\n",
      "16374/16374 [==============================] - 73s 4ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 1.6545 - val_acc: 0.7555\n",
      "Epoch 28/100\n",
      "16374/16374 [==============================] - 85s 5ms/step - loss: 0.0120 - acc: 0.9965 - val_loss: 1.6320 - val_acc: 0.7584\n",
      "Epoch 29/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0079 - acc: 0.9974 - val_loss: 1.5924 - val_acc: 0.7524\n",
      "Epoch 30/100\n",
      "16374/16374 [==============================] - 72s 4ms/step - loss: 0.0089 - acc: 0.9974 - val_loss: 1.6807 - val_acc: 0.7527\n",
      "Epoch 31/100\n",
      "16374/16374 [==============================] - 58s 4ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 1.7808 - val_acc: 0.7529\n",
      "Epoch 32/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 1.7459 - val_acc: 0.7459\n",
      "Epoch 33/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0076 - acc: 0.9977 - val_loss: 1.8671 - val_acc: 0.7531\n",
      "Epoch 34/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0080 - acc: 0.9977 - val_loss: 1.6293 - val_acc: 0.7495\n",
      "Epoch 35/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 1.7891 - val_acc: 0.7510\n",
      "Epoch 36/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 1.7569 - val_acc: 0.7458\n",
      "Epoch 37/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 1.6472 - val_acc: 0.7516\n",
      "Epoch 38/100\n",
      "16374/16374 [==============================] - 57s 3ms/step - loss: 0.0045 - acc: 0.9983 - val_loss: 1.8032 - val_acc: 0.7484\n",
      "Epoch 39/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0050 - acc: 0.9985 - val_loss: 1.8176 - val_acc: 0.7441\n",
      "Epoch 40/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 1.8972 - val_acc: 0.7485\n",
      "Epoch 41/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0026 - acc: 0.9993 - val_loss: 2.0094 - val_acc: 0.7505\n",
      "Epoch 42/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0041 - acc: 0.9989 - val_loss: 2.1253 - val_acc: 0.7308\n",
      "Epoch 43/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 1.9493 - val_acc: 0.7546\n",
      "Epoch 44/100\n",
      "16374/16374 [==============================] - 58s 4ms/step - loss: 0.0024 - acc: 0.9993 - val_loss: 1.8857 - val_acc: 0.7500\n",
      "Epoch 45/100\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 1.8420 - val_acc: 0.7490\n",
      "Epoch 46/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.9597 - val_acc: 0.7496\n",
      "Epoch 47/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0061 - acc: 0.9982 - val_loss: 1.9060 - val_acc: 0.7537\n",
      "Epoch 48/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0031 - acc: 0.9992 - val_loss: 2.1408 - val_acc: 0.7580\n",
      "Epoch 49/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 2.0056 - val_acc: 0.7530\n",
      "Epoch 50/100\n",
      "16374/16374 [==============================] - 69s 4ms/step - loss: 8.8532e-04 - acc: 0.9999 - val_loss: 2.0529 - val_acc: 0.7517\n",
      "Epoch 51/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 6.7414e-04 - acc: 0.9999 - val_loss: 2.1800 - val_acc: 0.7508\n",
      "Epoch 52/100\n",
      "16374/16374 [==============================] - 70s 4ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 1.9848 - val_acc: 0.7498\n",
      "Epoch 53/100\n",
      "16374/16374 [==============================] - 68s 4ms/step - loss: 0.0049 - acc: 0.9987 - val_loss: 2.0519 - val_acc: 0.7539\n",
      "Epoch 54/100\n",
      "16374/16374 [==============================] - 67s 4ms/step - loss: 0.0022 - acc: 0.9995 - val_loss: 1.9504 - val_acc: 0.7429\n",
      "Epoch 55/100\n",
      "16374/16374 [==============================] - 68s 4ms/step - loss: 0.0017 - acc: 0.9992 - val_loss: 2.1455 - val_acc: 0.7412\n",
      "Epoch 56/100\n",
      "16374/16374 [==============================] - 64s 4ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 1.9693 - val_acc: 0.7521\n",
      "Epoch 57/100\n",
      "16374/16374 [==============================] - 58s 4ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 2.0039 - val_acc: 0.7381\n",
      "Epoch 58/100\n",
      "16374/16374 [==============================] - 57s 3ms/step - loss: 8.9008e-04 - acc: 0.9998 - val_loss: 2.0564 - val_acc: 0.7502\n",
      "Epoch 59/100\n",
      "16374/16374 [==============================] - 55s 3ms/step - loss: 9.5139e-04 - acc: 0.9998 - val_loss: 2.0776 - val_acc: 0.7465\n",
      "Epoch 60/100\n",
      "16374/16374 [==============================] - 54s 3ms/step - loss: 6.8790e-04 - acc: 0.9998 - val_loss: 2.1763 - val_acc: 0.7492\n",
      "Epoch 61/100\n",
      "16374/16374 [==============================] - 54s 3ms/step - loss: 9.7910e-04 - acc: 0.9997 - val_loss: 2.2310 - val_acc: 0.7546\n",
      "Epoch 62/100\n",
      "16374/16374 [==============================] - 55s 3ms/step - loss: 7.5251e-04 - acc: 0.9997 - val_loss: 2.2583 - val_acc: 0.7491\n",
      "Epoch 63/100\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 9.9441e-04 - acc: 0.9995 - val_loss: 2.2304 - val_acc: 0.7476\n",
      "Epoch 64/100\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 2.2469 - val_acc: 0.7544\n",
      "Epoch 65/100\n",
      "16374/16374 [==============================] - 68s 4ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 2.1006 - val_acc: 0.7495\n",
      "Epoch 66/100\n",
      "16374/16374 [==============================] - 70s 4ms/step - loss: 0.0021 - acc: 0.9994 - val_loss: 2.0793 - val_acc: 0.7511\n",
      "Epoch 67/100\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 0.0021 - acc: 0.9991 - val_loss: 2.0790 - val_acc: 0.7516\n",
      "Epoch 68/100\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.0012 - acc: 0.9997 - val_loss: 2.0933 - val_acc: 0.7486\n",
      "Epoch 69/100\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 3.0354e-04 - acc: 0.9999 - val_loss: 2.2802 - val_acc: 0.7496\n",
      "Epoch 70/100\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 5.7698e-04 - acc: 0.9998 - val_loss: 2.2501 - val_acc: 0.7494\n",
      "Epoch 71/100\n",
      "16374/16374 [==============================] - 64s 4ms/step - loss: 7.9109e-04 - acc: 0.9999 - val_loss: 2.2693 - val_acc: 0.7517\n",
      "Epoch 72/100\n",
      "16374/16374 [==============================] - 71s 4ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 2.2345 - val_acc: 0.7506\n",
      "Epoch 73/100\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 2.1371 - val_acc: 0.7429\n",
      "Epoch 74/100\n",
      "16374/16374 [==============================] - 65s 4ms/step - loss: 4.0911e-04 - acc: 0.9999 - val_loss: 2.2443 - val_acc: 0.7524\n",
      "Epoch 75/100\n",
      "16374/16374 [==============================] - 87s 5ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 2.2689 - val_acc: 0.7481\n",
      "Epoch 76/100\n",
      "16374/16374 [==============================] - 71s 4ms/step - loss: 0.0018 - acc: 0.9996 - val_loss: 2.0934 - val_acc: 0.7505\n",
      "Epoch 77/100\n",
      "16374/16374 [==============================] - 72s 4ms/step - loss: 6.5400e-04 - acc: 0.9998 - val_loss: 2.1737 - val_acc: 0.7505\n",
      "Epoch 78/100\n",
      "16374/16374 [==============================] - 69s 4ms/step - loss: 6.9208e-04 - acc: 0.9997 - val_loss: 2.3442 - val_acc: 0.7367\n",
      "Epoch 79/100\n",
      "16374/16374 [==============================] - 81s 5ms/step - loss: 8.1015e-04 - acc: 0.9999 - val_loss: 2.3349 - val_acc: 0.7480\n",
      "Epoch 80/100\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 2.1446 - val_acc: 0.7451\n",
      "Epoch 81/100\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 0.0010 - acc: 0.9998 - val_loss: 2.2619 - val_acc: 0.7290\n",
      "Epoch 82/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 2.2240 - val_acc: 0.7484\n",
      "Epoch 83/100\n",
      "16374/16374 [==============================] - 72s 4ms/step - loss: 6.7874e-04 - acc: 0.9998 - val_loss: 2.2608 - val_acc: 0.7491\n",
      "Epoch 84/100\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 9.1126e-04 - acc: 0.9998 - val_loss: 2.3641 - val_acc: 0.7481\n",
      "Epoch 85/100\n",
      "16374/16374 [==============================] - 67s 4ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 2.0893 - val_acc: 0.7446\n",
      "Epoch 86/100\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 2.2217 - val_acc: 0.7498\n",
      "Epoch 87/100\n",
      "16374/16374 [==============================] - 57s 3ms/step - loss: 8.3139e-04 - acc: 0.9998 - val_loss: 2.1820 - val_acc: 0.7479\n",
      "Epoch 88/100\n",
      "16374/16374 [==============================] - 57s 3ms/step - loss: 1.3667e-04 - acc: 1.0000 - val_loss: 2.2532 - val_acc: 0.7491\n",
      "Epoch 89/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0018 - acc: 0.9993 - val_loss: 2.1529 - val_acc: 0.7518\n",
      "Epoch 90/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 3.5482e-04 - acc: 0.9999 - val_loss: 2.1566 - val_acc: 0.7476\n",
      "Epoch 91/100\n",
      "16374/16374 [==============================] - 57s 3ms/step - loss: 8.1658e-05 - acc: 1.0000 - val_loss: 2.2543 - val_acc: 0.7486\n",
      "Epoch 92/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 1.6145e-04 - acc: 0.9999 - val_loss: 2.2436 - val_acc: 0.7477\n",
      "Epoch 93/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 9.0240e-04 - acc: 0.9996 - val_loss: 2.3353 - val_acc: 0.7485\n",
      "Epoch 94/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 9.6695e-04 - acc: 0.9995 - val_loss: 2.4020 - val_acc: 0.7459\n",
      "Epoch 95/100\n",
      "16374/16374 [==============================] - 56s 3ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 2.2134 - val_acc: 0.7442\n",
      "Epoch 96/100\n",
      "16374/16374 [==============================] - 65s 4ms/step - loss: 0.0023 - acc: 0.9996 - val_loss: 2.1124 - val_acc: 0.7454\n",
      "Epoch 97/100\n",
      "16374/16374 [==============================] - 69s 4ms/step - loss: 2.5818e-04 - acc: 0.9999 - val_loss: 2.2990 - val_acc: 0.7483\n",
      "Epoch 98/100\n",
      "16374/16374 [==============================] - 77s 5ms/step - loss: 0.0012 - acc: 0.9998 - val_loss: 2.1563 - val_acc: 0.7472\n",
      "Epoch 99/100\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 3.1808e-04 - acc: 0.9999 - val_loss: 2.2092 - val_acc: 0.7448\n",
      "Epoch 100/100\n",
      "16374/16374 [==============================] - 78s 5ms/step - loss: 1.5954e-04 - acc: 1.0000 - val_loss: 2.3055 - val_acc: 0.7516\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.loc[:,['stars','text']]\n",
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n",
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3.0 else 0)\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=100)\n",
    "\n",
    "\n",
    "df_save = pd.DataFrame(data)\n",
    "df_label = pd.DataFrame(np.array(labels))\n",
    "\n",
    "result = pd.concat([df_save, df_label], axis = 1)\n",
    "result.to_csv('train_dense_word_vectors.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', sep =None, names = ['stars', 'text'], error_bad_lines=False, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "#df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "#df = df[df.text.apply(lambda x: x !=\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Mr Hoagie is an institution. Walking in, it do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent food. Superb customer service. I mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes this place is a little out dated and not o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>We checked this place out this past Monday for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Wing sauce is like water. Pretty much a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Cold cheap beer. Good bar food. Good service. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I highly recommend this place. The mechanics a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I am a big believer in first impressions, so w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Decent range somewhat close to the city.  The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Owning a driving range inside the city limits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This place is absolute garbage...  Half of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Before I finally made it over to this range I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I drove by yesterday to get a sneak peak.  It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>THANK YOU ROB! i truly appreciated all the hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>After waiting for almost 30 minutes to trade i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I visited this store several months ago to sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>My fianc upgraded his phone at the Apple stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Waited over 45 mins, 2 people ahead of me and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This place was DELICIOUS!!  My parents saw a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Can't miss stop for the best Fish Sandwich in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This place should have a lot more reviews - bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>This place was very good. I found out about Em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Old school.....traditional \"mom 'n pop\" qualit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Seen this restaurant on 25 best places in Pitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Like a lot of recent patrons, I heard about Em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Wonderful reuben.  Map shown on Yelp page is i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.0</td>\n",
       "      <td>After a morning of Thrift Store hunting, a fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.0</td>\n",
       "      <td>This is a hidden gem, no really. It took us fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>From reading the other reviews, I see this pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27261</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Great as usual this a most for southern food. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27262</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Old fashioned cafe. Simple meat and three menu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27263</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Good food good people great place breakfast al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27264</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This is my favorite place for breakfast!  Havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27265</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Never disappointed in Circle G! It has been a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27266</th>\n",
       "      <td>4.0</td>\n",
       "      <td>This spot is near my work &amp; I usually eat here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27267</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I can't believe no one has reviewed this place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27268</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Hey you know that sports bar with a just-okay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27269</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Pros: Good food, good atmosphere, polite staff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27270</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I have been here many times over the years and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27271</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Sports Page is a West side classic. I have bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ive heard nothing but good things about this r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>3.0</td>\n",
       "      <td>The several times I've been here or gotten foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27274</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes the decor is dated, yes the food isnt amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27275</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I went to this location as part of a large par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27276</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Im not sure how this place could not get a hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27277</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Good food, good service, and just a great plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27278</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Food is just OK, high turn over of employees w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27279</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Great breakfast - eggs, scrambles, whatever......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27280</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Today my co-workers dragged me to another plac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27281</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I usually eat here during my lunch break. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27282</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Best breakfast in the area. Good food. Good se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27283</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I'm a big fan of this place and I visit stocky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27284</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This place is outstanding and worth finding!  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27285</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Ok I figured this would be ordinary diner food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27286</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Love the food and the atmosphere. I eat here a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27287</th>\n",
       "      <td>3.0</td>\n",
       "      <td>The service was nice, the front desk girls wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27288</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Horrible showers, but nice folks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27289</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Stopped here on the way to Fort Jackson.  It's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>So, I had to play hotel switcheroo because Hya...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27291 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars                                               text\n",
       "0        4.0  Mr Hoagie is an institution. Walking in, it do...\n",
       "1        5.0  Excellent food. Superb customer service. I mis...\n",
       "2        5.0  Yes this place is a little out dated and not o...\n",
       "3        5.0  All the food is great here. But the best thing...\n",
       "4        3.0  We checked this place out this past Monday for...\n",
       "5        1.0  Wing sauce is like water. Pretty much a lot of...\n",
       "6        4.0  Cold cheap beer. Good bar food. Good service. ...\n",
       "7        5.0  I highly recommend this place. The mechanics a...\n",
       "8        5.0  I am a big believer in first impressions, so w...\n",
       "9        3.0  Decent range somewhat close to the city.  The ...\n",
       "10       1.0  Owning a driving range inside the city limits ...\n",
       "11       1.0  This place is absolute garbage...  Half of the...\n",
       "12       4.0  Before I finally made it over to this range I ...\n",
       "13       4.0  I drove by yesterday to get a sneak peak.  It ...\n",
       "14       5.0  THANK YOU ROB! i truly appreciated all the hel...\n",
       "15       2.0  After waiting for almost 30 minutes to trade i...\n",
       "16       4.0  I visited this store several months ago to sim...\n",
       "17       1.0  My fianc upgraded his phone at the Apple stor...\n",
       "18       1.0  Waited over 45 mins, 2 people ahead of me and ...\n",
       "19       5.0  This place was DELICIOUS!!  My parents saw a r...\n",
       "20       5.0  Can't miss stop for the best Fish Sandwich in ...\n",
       "21       5.0  This place should have a lot more reviews - bu...\n",
       "22       4.0  This place was very good. I found out about Em...\n",
       "23       5.0  Old school.....traditional \"mom 'n pop\" qualit...\n",
       "24       5.0  Seen this restaurant on 25 best places in Pitt...\n",
       "25       5.0  Like a lot of recent patrons, I heard about Em...\n",
       "26       4.0  Wonderful reuben.  Map shown on Yelp page is i...\n",
       "27       5.0  After a morning of Thrift Store hunting, a fri...\n",
       "28       4.0  This is a hidden gem, no really. It took us fo...\n",
       "29       5.0  From reading the other reviews, I see this pla...\n",
       "...      ...                                                ...\n",
       "27261    5.0  Great as usual this a most for southern food. ...\n",
       "27262    3.0  Old fashioned cafe. Simple meat and three menu...\n",
       "27263    4.0  Good food good people great place breakfast al...\n",
       "27264    5.0  This is my favorite place for breakfast!  Havi...\n",
       "27265    5.0  Never disappointed in Circle G! It has been a ...\n",
       "27266    4.0  This spot is near my work & I usually eat here...\n",
       "27267    4.0  I can't believe no one has reviewed this place...\n",
       "27268    4.0  Hey you know that sports bar with a just-okay ...\n",
       "27269    4.0  Pros: Good food, good atmosphere, polite staff...\n",
       "27270    4.0  I have been here many times over the years and...\n",
       "27271    4.0  Sports Page is a West side classic. I have bee...\n",
       "27272    1.0  ive heard nothing but good things about this r...\n",
       "27273    3.0  The several times I've been here or gotten foo...\n",
       "27274    4.0  Yes the decor is dated, yes the food isnt amaz...\n",
       "27275    1.0  I went to this location as part of a large par...\n",
       "27276    4.0  Im not sure how this place could not get a hig...\n",
       "27277    4.0  Good food, good service, and just a great plac...\n",
       "27278    1.0  Food is just OK, high turn over of employees w...\n",
       "27279    4.0  Great breakfast - eggs, scrambles, whatever......\n",
       "27280    3.0  Today my co-workers dragged me to another plac...\n",
       "27281    5.0  I usually eat here during my lunch break. The ...\n",
       "27282    5.0  Best breakfast in the area. Good food. Good se...\n",
       "27283    5.0  I'm a big fan of this place and I visit stocky...\n",
       "27284    5.0  This place is outstanding and worth finding!  ...\n",
       "27285    5.0  Ok I figured this would be ordinary diner food...\n",
       "27286    5.0  Love the food and the atmosphere. I eat here a...\n",
       "27287    3.0  The service was nice, the front desk girls wer...\n",
       "27288    2.0                  Horrible showers, but nice folks.\n",
       "27289    1.0  Stopped here on the way to Fort Jackson.  It's...\n",
       "27290    NaN  So, I had to play hotel switcheroo because Hya...\n",
       "\n",
       "[27291 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['stars','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>votes.cool</th>\n",
       "      <th>business_id</th>\n",
       "      <th>votes.funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>votes.useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUFPaY9KxDAcGqfsorJp3Q</td>\n",
       "      <td>Ya85v4eqdd6k9Od8HbQjyA</td>\n",
       "      <td>Mr Hoagie is an institution. Walking in, it do...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8/1/2012</td>\n",
       "      <td>review</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iu6AxdBYGR4A0wspR9BYHA</td>\n",
       "      <td>KPvLNJ21_4wbYNctrOwWdQ</td>\n",
       "      <td>Excellent food. Superb customer service. I mis...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2/13/2014</td>\n",
       "      <td>review</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auESFwWvW42h6alXgFxAXQ</td>\n",
       "      <td>fFSoGV46Yxuwbr3fHNuZig</td>\n",
       "      <td>Yes this place is a little out dated and not o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10/31/2015</td>\n",
       "      <td>review</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uK8tzraOp4M5u3uYrqIBXg</td>\n",
       "      <td>Di3exaUCFNw1V4kSNW5pgA</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UsFtqoBl7naz8AVUBZMjQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11/8/2013</td>\n",
       "      <td>review</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I_47G-R2_egp7ME5u_ltew</td>\n",
       "      <td>0Lua2-PbqEQMjD9r89-asw</td>\n",
       "      <td>We checked this place out this past Monday for...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UsFtqoBl7naz8AVUBZMjQQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3/29/2014</td>\n",
       "      <td>review</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id               review_id  \\\n",
       "0  PUFPaY9KxDAcGqfsorJp3Q  Ya85v4eqdd6k9Od8HbQjyA   \n",
       "1  Iu6AxdBYGR4A0wspR9BYHA  KPvLNJ21_4wbYNctrOwWdQ   \n",
       "2  auESFwWvW42h6alXgFxAXQ  fFSoGV46Yxuwbr3fHNuZig   \n",
       "3  uK8tzraOp4M5u3uYrqIBXg  Di3exaUCFNw1V4kSNW5pgA   \n",
       "4  I_47G-R2_egp7ME5u_ltew  0Lua2-PbqEQMjD9r89-asw   \n",
       "\n",
       "                                                text  votes.cool  \\\n",
       "0  Mr Hoagie is an institution. Walking in, it do...         0.0   \n",
       "1  Excellent food. Superb customer service. I mis...         0.0   \n",
       "2  Yes this place is a little out dated and not o...         0.0   \n",
       "3  All the food is great here. But the best thing...         0.0   \n",
       "4  We checked this place out this past Monday for...         0.0   \n",
       "\n",
       "              business_id  votes.funny  stars        date    type  \\\n",
       "0  5UmKMjUEUNdYWqANhGckJw          0.0    4.0    8/1/2012  review   \n",
       "1  5UmKMjUEUNdYWqANhGckJw          0.0    5.0   2/13/2014  review   \n",
       "2  5UmKMjUEUNdYWqANhGckJw          0.0    5.0  10/31/2015  review   \n",
       "3  UsFtqoBl7naz8AVUBZMjQQ          0.0    5.0   11/8/2013  review   \n",
       "4  UsFtqoBl7naz8AVUBZMjQQ          0.0    3.0   3/29/2014  review   \n",
       "\n",
       "   votes.useful  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3.0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to C:\\Users\n",
      "[nltk_data]    |     \\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to C:\\Users\\m\n",
      "[nltk_data]    |     uhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\muhammadabunaser\\A\n",
      "[nltk_data]    |     ppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to C:\\Use\n",
      "[nltk_data]    |     rs\\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to \n",
      "[nltk_data]    |     C:\\Users\\muhammadabunaser\\AppData\\Roaming\\nltk_da\n",
      "[nltk_data]    |     ta...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to C:\\Users\\\n",
      "[nltk_data]    |     muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "(27290, 50)\n",
      "Train on 16374 samples, validate on 10916 samples\n",
      "Epoch 1/10\n",
      "16374/16374 [==============================] - 83s 5ms/step - loss: 0.4751 - acc: 0.7732 - val_loss: 0.4241 - val_acc: 0.8046\n",
      "Epoch 2/10\n",
      "16374/16374 [==============================] - 85s 5ms/step - loss: 0.3272 - acc: 0.8649 - val_loss: 0.4410 - val_acc: 0.7989\n",
      "Epoch 3/10\n",
      "16374/16374 [==============================] - 84s 5ms/step - loss: 0.2565 - acc: 0.8961 - val_loss: 0.4799 - val_acc: 0.7984\n",
      "Epoch 4/10\n",
      "16374/16374 [==============================] - 86s 5ms/step - loss: 0.1993 - acc: 0.9233 - val_loss: 0.5722 - val_acc: 0.7834\n",
      "Epoch 5/10\n",
      "16374/16374 [==============================] - 86s 5ms/step - loss: 0.1555 - acc: 0.9436 - val_loss: 0.6029 - val_acc: 0.7784\n",
      "Epoch 6/10\n",
      "16374/16374 [==============================] - 84s 5ms/step - loss: 0.1302 - acc: 0.9534 - val_loss: 0.7101 - val_acc: 0.7778\n",
      "Epoch 7/10\n",
      "16374/16374 [==============================] - 80s 5ms/step - loss: 0.1107 - acc: 0.9609 - val_loss: 0.7884 - val_acc: 0.7667\n",
      "Epoch 8/10\n",
      "16374/16374 [==============================] - 77s 5ms/step - loss: 0.0857 - acc: 0.9687 - val_loss: 0.8630 - val_acc: 0.7687\n",
      "Epoch 9/10\n",
      "16374/16374 [==============================] - 75s 5ms/step - loss: 0.0754 - acc: 0.9729 - val_loss: 0.9803 - val_acc: 0.7593\n",
      "Epoch 10/10\n",
      "16374/16374 [==============================] - 77s 5ms/step - loss: 0.0659 - acc: 0.9770 - val_loss: 0.9723 - val_acc: 0.7594\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_csv('train_final.csv')\n",
    "df.loc[:,['stars','text']]\n",
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n",
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3 else 0)\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=10)\n",
    "\n",
    "\n",
    "df_save = pd.DataFrame(data)\n",
    "df_label = pd.DataFrame(np.array(labels))\n",
    "\n",
    "result = pd.concat([df_save, df_label], axis = 1)\n",
    "result.to_csv('train_dense_word_vectors1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to C:\\Users\n",
      "[nltk_data]    |     \\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to C:\\Users\\m\n",
      "[nltk_data]    |     uhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\muhammadabunaser\\A\n",
      "[nltk_data]    |     ppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to C:\\Use\n",
      "[nltk_data]    |     rs\\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to \n",
      "[nltk_data]    |     C:\\Users\\muhammadabunaser\\AppData\\Roaming\\nltk_da\n",
      "[nltk_data]    |     ta...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to C:\\Users\\\n",
      "[nltk_data]    |     muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27290, 50)\n",
      "Train on 16374 samples, validate on 10916 samples\n",
      "Epoch 1/10\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.4715 - acc: 0.7796 - val_loss: 0.4383 - val_acc: 0.7980\n",
      "Epoch 2/10\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.3280 - acc: 0.8634 - val_loss: 0.4625 - val_acc: 0.7966\n",
      "Epoch 3/10\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.2567 - acc: 0.8986 - val_loss: 0.4921 - val_acc: 0.7935\n",
      "Epoch 4/10\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.1917 - acc: 0.9272 - val_loss: 0.5692 - val_acc: 0.7864\n",
      "Epoch 5/10\n",
      "16374/16374 [==============================] - 72s 4ms/step - loss: 0.1544 - acc: 0.9414 - val_loss: 0.6563 - val_acc: 0.7812\n",
      "Epoch 6/10\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 0.1313 - acc: 0.9532 - val_loss: 0.7143 - val_acc: 0.7743\n",
      "Epoch 7/10\n",
      "16374/16374 [==============================] - 71s 4ms/step - loss: 0.1052 - acc: 0.9619 - val_loss: 0.8273 - val_acc: 0.7733\n",
      "Epoch 8/10\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.0886 - acc: 0.9689 - val_loss: 0.9058 - val_acc: 0.7732\n",
      "Epoch 9/10\n",
      "16374/16374 [==============================] - 62s 4ms/step - loss: 0.0762 - acc: 0.9745 - val_loss: 0.9789 - val_acc: 0.7657\n",
      "Epoch 10/10\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.0509 - acc: 0.9831 - val_loss: 1.1386 - val_acc: 0.7666\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_csv('train_final.csv')\n",
    "df.loc[:,['stars','text']]\n",
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n",
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3 else 0)\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "data = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=10)\n",
    "\n",
    "\n",
    "#df_save = pd.DataFrame(data)\n",
    "df_label = pd.DataFrame(np.array(labels))\n",
    "\n",
    "result = pd.concat([df_label], axis = 1)\n",
    "result.to_csv('train_dense_word_vectors2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to C:\\Users\n",
      "[nltk_data]    |     \\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to C:\\Users\\m\n",
      "[nltk_data]    |     uhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\muhammadabunaser\\A\n",
      "[nltk_data]    |     ppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to C:\\Use\n",
      "[nltk_data]    |     rs\\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to \n",
      "[nltk_data]    |     C:\\Users\\muhammadabunaser\\AppData\\Roaming\\nltk_da\n",
      "[nltk_data]    |     ta...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to C:\\Users\\\n",
      "[nltk_data]    |     muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "(27290, 50)\n",
      "Train on 16374 samples, validate on 10916 samples\n",
      "Epoch 1/20\n",
      "16374/16374 [==============================] - 84s 5ms/step - loss: 0.4688 - acc: 0.7803 - val_loss: 0.4345 - val_acc: 0.8039\n",
      "Epoch 2/20\n",
      "16374/16374 [==============================] - 114s 7ms/step - loss: 0.3274 - acc: 0.8637 - val_loss: 0.4395 - val_acc: 0.8023\n",
      "Epoch 3/20\n",
      "16374/16374 [==============================] - 105s 6ms/step - loss: 0.2573 - acc: 0.8969 - val_loss: 0.5077 - val_acc: 0.7948\n",
      "Epoch 4/20\n",
      "16374/16374 [==============================] - 107s 7ms/step - loss: 0.1964 - acc: 0.9251 - val_loss: 0.5852 - val_acc: 0.7875\n",
      "Epoch 5/20\n",
      "16374/16374 [==============================] - 118s 7ms/step - loss: 0.1479 - acc: 0.9445 - val_loss: 0.7681 - val_acc: 0.7844\n",
      "Epoch 6/20\n",
      "16374/16374 [==============================] - 85s 5ms/step - loss: 0.1280 - acc: 0.9527 - val_loss: 0.6756 - val_acc: 0.7825\n",
      "Epoch 7/20\n",
      "16374/16374 [==============================] - 94s 6ms/step - loss: 0.1068 - acc: 0.9621 - val_loss: 0.8242 - val_acc: 0.7721\n",
      "Epoch 8/20\n",
      "16374/16374 [==============================] - 79s 5ms/step - loss: 0.0885 - acc: 0.9691 - val_loss: 0.8547 - val_acc: 0.7646\n",
      "Epoch 9/20\n",
      "16374/16374 [==============================] - 100s 6ms/step - loss: 0.0718 - acc: 0.9748 - val_loss: 0.8657 - val_acc: 0.7680\n",
      "Epoch 10/20\n",
      "16374/16374 [==============================] - 91s 6ms/step - loss: 0.0579 - acc: 0.9792 - val_loss: 1.0207 - val_acc: 0.7626\n",
      "Epoch 11/20\n",
      "16374/16374 [==============================] - 85s 5ms/step - loss: 0.0513 - acc: 0.9822 - val_loss: 1.0142 - val_acc: 0.7538\n",
      "Epoch 12/20\n",
      "16374/16374 [==============================] - 72s 4ms/step - loss: 0.0420 - acc: 0.9861 - val_loss: 1.1681 - val_acc: 0.7625\n",
      "Epoch 13/20\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.0381 - acc: 0.9883 - val_loss: 1.0952 - val_acc: 0.7602\n",
      "Epoch 14/20\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.0304 - acc: 0.9904 - val_loss: 1.2132 - val_acc: 0.7559\n",
      "Epoch 15/20\n",
      "16374/16374 [==============================] - 58s 4ms/step - loss: 0.0262 - acc: 0.9909 - val_loss: 1.1828 - val_acc: 0.7574\n",
      "Epoch 16/20\n",
      "16374/16374 [==============================] - 59s 4ms/step - loss: 0.0229 - acc: 0.9934 - val_loss: 1.3741 - val_acc: 0.7515\n",
      "Epoch 17/20\n",
      "16374/16374 [==============================] - 63s 4ms/step - loss: 0.0211 - acc: 0.9923 - val_loss: 1.3536 - val_acc: 0.7591\n",
      "Epoch 18/20\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.0215 - acc: 0.9930 - val_loss: 1.3065 - val_acc: 0.7359\n",
      "Epoch 19/20\n",
      "16374/16374 [==============================] - 60s 4ms/step - loss: 0.0191 - acc: 0.9940 - val_loss: 1.2705 - val_acc: 0.7469\n",
      "Epoch 20/20\n",
      "16374/16374 [==============================] - 61s 4ms/step - loss: 0.0152 - acc: 0.9955 - val_loss: 1.4680 - val_acc: 0.7616\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_csv('train_final.csv')\n",
    "df.loc[:,['stars','text']]\n",
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]  list-like series\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n",
    "labels = df['stars'].map(lambda x : 1 if int(x) > 3 else 0) # review comments are classified into two class: either 1 or 0.\n",
    "\n",
    "# Reviews that have stars higher than three are considered as 1 while the reviews with stars less than or equal to 3 are negative.\n",
    "\n",
    "#the problem is a supervised learning problem\n",
    "\n",
    "\n",
    "##### First tokenize the text and convert them to sequences ###\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000   # Using the top 20000 unique words because of computational expenses.\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size) # Tokenize the comments\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])  # Convert the comments into Sequences\n",
    "data = pad_sequences(sequences, maxlen=50) # Limit the number of words in each comment is 50 words\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50)) # Starts with embedding layer.\n",
    "#The task of embedding layer is to let the system expand each token to a more massive vector, allowing the network\n",
    "# to represent a word in a meaningful way\n",
    "# The layer takes 20000 as the first argument, which is the size of the vocabulary\n",
    "# the second input parameter is the dimension of the embeddings\n",
    "#The third is the input length of 50, which is the length of each comment sequence\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=20) #Split the datasets as 60% training and 40% for validation\n",
    "\n",
    "\n",
    "#df_save = pd.DataFrame(data)\n",
    "df_label = pd.DataFrame(np.array(labels))\n",
    "\n",
    "#result1 = pd.concat(['text','stars'], axis = 1)\n",
    "result = pd.concat([df_label], axis = 1)\n",
    "result.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to C:\\Users\n",
      "[nltk_data]    |     \\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to C:\\Users\\m\n",
      "[nltk_data]    |     uhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\muhammadabunaser\\A\n",
      "[nltk_data]    |     ppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\muhammadabunaser\\\n",
      "[nltk_data]    |     AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\muhammadabunas\n",
      "[nltk_data]    |     er\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to C:\\Use\n",
      "[nltk_data]    |     rs\\muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\muhammadabunaser\n",
      "[nltk_data]    |     \\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to C:\\Users\\muh\n",
      "[nltk_data]    |     ammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\muhammadabunase\n",
      "[nltk_data]    |     r\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\muhammadabuna\n",
      "[nltk_data]    |     ser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to C:\\Users\\muha\n",
      "[nltk_data]    |     mmadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\muham\n",
      "[nltk_data]    |     madabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\muhamm\n",
      "[nltk_data]    |     adabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\muhammadabu\n",
      "[nltk_data]    |     naser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to C:\\\n",
      "[nltk_data]    |     Users\\muhammadabunaser\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]    |     ..\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to \n",
      "[nltk_data]    |     C:\\Users\\muhammadabunaser\\AppData\\Roaming\\nltk_da\n",
      "[nltk_data]    |     ta...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\muhammad\n",
      "[nltk_data]    |     abunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to C:\\Users\\\n",
      "[nltk_data]    |     muhammadabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\muhamma\n",
      "[nltk_data]    |     dabunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\muhammada\n",
      "[nltk_data]    |     bunaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\muhammadab\n",
      "[nltk_data]    |     unaser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\muhammadabun\n",
      "[nltk_data]    |     aser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27290, 50)\n",
      "Train on 16374 samples, validate on 10916 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 4 which is outside the valid range of [0, 1).  Label values: 1 3 2 1 3 1 1 1 3 3 3 2 1 2 4 1 1 4 4 1 1 2 3 2 3 2 1 2 4 4 4 4\n\t [[Node: loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_2/dense_3_loss/Reshape_1, loss_2/dense_3_loss/Cast)]]\n\nCaused by op 'loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-fddc4dc0773a>\", line 120, in <module>\n    model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\models.py\", line 821, in compile\n    **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 459, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 62, in sparse_categorical_crossentropy\n    return K.sparse_categorical_crossentropy(y_true, y_pred)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2922, in sparse_categorical_crossentropy\n    logits=logits)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 4546, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 4 which is outside the valid range of [0, 1).  Label values: 1 3 2 1 3 1 1 1 3 3 3 2 1 2 4 1 1 4 4 1 1 2 3 2 3 2 1 2 4 4 4 4\n\t [[Node: loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_2/dense_3_loss/Reshape_1, loss_2/dense_3_loss/Cast)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Received a label value of 4 which is outside the valid range of [0, 1).  Label values: 1 3 2 1 3 1 1 1 3 3 3 2 1 2 4 1 1 4 4 1 1 2 3 2 3 2 1 2 4 4 4 4\n\t [[Node: loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_2/dense_3_loss/Reshape_1, loss_2/dense_3_loss/Cast)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fddc4dc0773a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mmodel_lstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Split the datasets as 60% training and 40% for validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Received a label value of 4 which is outside the valid range of [0, 1).  Label values: 1 3 2 1 3 1 1 1 3 3 3 2 1 2 4 1 1 4 4 1 1 2 3 2 3 2 1 2 4 4 4 4\n\t [[Node: loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_2/dense_3_loss/Reshape_1, loss_2/dense_3_loss/Cast)]]\n\nCaused by op 'loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-fddc4dc0773a>\", line 120, in <module>\n    model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\models.py\", line 821, in compile\n    **kwargs)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in compile\n    sample_weight, mask)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 459, in weighted\n    score_array = fn(y_true, y_pred)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 62, in sparse_categorical_crossentropy\n    return K.sparse_categorical_crossentropy(y_true, y_pred)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2922, in sparse_categorical_crossentropy\n    logits=logits)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 4546, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\muhammadabunaser\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Received a label value of 4 which is outside the valid range of [0, 1).  Label values: 1 3 2 1 3 1 1 1 3 3 3 2 1 2 4 1 1 4 4 1 1 2 3 2 3 2 1 2 4 4 4 4\n\t [[Node: loss_2/dense_3_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss_2/dense_3_loss/Reshape_1, loss_2/dense_3_loss/Cast)]]\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "## Plot\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import matplotlib as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#import nltk\n",
    "#nltk.download_shell()\n",
    "nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Other\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.read_csv('train_final.csv')\n",
    "df.loc[:,['stars','text']]\n",
    "\n",
    "df= df.dropna()\n",
    "#df = df[df.stars.apply(lambda x: x.isnumeric())]  list-like series\n",
    "df = df[df.stars.apply(lambda x: x !=\"\")]\n",
    "df = df[df.text.apply(lambda x: x !=\"\")]\n",
    "\n",
    "#labels = df['stars'].map(lambda x : 1 if int(x) > 3 else 0) # review comments are classified into two class: either 1 or 0.\n",
    "\n",
    "labels = df['stars'].map(lambda x : random.uniform(2,5) if int(x) > 3 else random.uniform(1,2))\n",
    "\n",
    "# Reviews that have stars higher than three are considered as 1 while the reviews with stars less than or equal to 3 are negative.\n",
    "\n",
    "#the problem is a supervised learning problem\n",
    "\n",
    "\n",
    "##### First tokenize the text and convert them to sequences ###\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))\n",
    "\n",
    "vocabulary_size = 20000   # Using the top 20000 unique words because of computational expenses.\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size) # Tokenize the comments\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])  # Convert the comments into Sequences\n",
    "data = pad_sequences(sequences, maxlen=50) # Limit the number of words in each comment is 50 words\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50)) # Starts with embedding layer.\n",
    "#The task of embedding layer is to let the system expand each token to a more massive vector, allowing the network\n",
    "# to represent a word in a meaningful way\n",
    "# The layer takes 20000 as the first argument, which is the size of the vocabulary\n",
    "# the second input parameter is the dimension of the embeddings\n",
    "#The third is the input length of 50, which is the length of each comment sequence\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(data, np.array(labels), validation_split=0.4, epochs=5) #Split the datasets as 60% training and 40% for validation\n",
    "\n",
    "\n",
    "#df_save = pd.DataFrame(data)\n",
    "df_label = pd.DataFrame(np.array(labels))\n",
    "\n",
    "#result1 = pd.concat(['text','stars'], axis = 1)\n",
    "result = pd.concat([df_label], axis = 1)\n",
    "result.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
